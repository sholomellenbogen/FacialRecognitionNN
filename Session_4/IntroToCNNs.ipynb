{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) are a special architecture of neural network that is meant to be used when the features have some local correlation. The most obvious example is for natural images; a pixel's neighbours are highly correlated with it, and can provide extra information about how to classify that pixel. Another example is for natural language processing. In this case, the words surrounding a word of interest offer extra contextual information about how to interpret that word. A similar thing happens with time series, or signal analysis.\n",
    "\n",
    "To accomplish this a convolutional neural networks uses a mathematical operation called a convolution. This basically amounts to taking sliding one function or matrix across another and doing elementwise multiplication and then accumulation. I actually really recommend looking at the Wikipedia page on [convolution](https://en.wikipedia.org/wiki/Convolution) (specially the gifs!). Crucially the matrix we'll be \"sliding\" will be the feature that we are looking for in the image.\n",
    "\n",
    "Convolutional neural networks are not just about convolutions, they also use _pooling_ operations to summarize data from one layer to another and reduce the dimensionality. They also make use of multi-layer perceptrons near the end of the network to accurately classify inputs.\n",
    "\n",
    "It should be mentioned that CNNs were heavily inspired by how the mammalian visual system is layed out and works. It is for this reason that you will sometimes see a lot of biologically inspired language around the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Again, I really recommend taking a look at the Wikipedia page for this to get a visual idea for how this operation works.\n",
    "\n",
    "Let's take the example of an image being analyzed by our neural network. Our network's job is to learn what features are important to use in a classification scenario. For example in the first layer of the CNN it is usually useful to learn features for basic contours (straight lines, end stops, right hand curves, etc). We sometimes call these features kernels, especially in the context of the convolution operation for historical reasons.\n",
    "\n",
    "![](fig91.png)\n",
    "\n",
    "The output of our convolution operation will be to the element-wise multiplication of our kernel with the image, and then accumulate all those values. Take a look at the image above.\n",
    "\n",
    "## Local Connections & Weight Sharing\n",
    "\n",
    "Now we need to figure out how to compute a convolution using a neural network. The key aspect here is that we will not want neurons to make connections to every neuron in the next layer. Instead we will want the neurons to form _local connections_. That is, the neuron will only make connections to the nearest k neighbours in the previous layer. This is represented by the top diagram in the image below. Contrast it to the fully connected layers in the bottom diagram.\n",
    "\n",
    "![](fig92.png)\n",
    "\n",
    "For the locally connect layers, the input $x_3$ only influences neurons $s_2, s_3$ and $s_4$. Or conversely the output neuron $s_3$ only takes inputs from $x_2, x_3$ and $x_4$. Perhaps it is easier to see why this is so important by looking at which neurons don't make connections. In the locally connected layers, ouput neuron $s_5$ does not receive any information from input neuron $x_3$. That is $s_5$ does not know, or care, about the value of $x_3$.\n",
    "\n",
    "Using these local connections means that we can drastically reduce the number of weights connecting one layer to another, which makes our network more efficient. This mimics a real biological fact in the early visual system called a neuron's _receptive field_, ie the area of the visual field that the neuron is actually paying attention to. In any case, the reason why we can drastically cut so many weights from our network is because we believe that the features we're ignoring are not important for the accuracy of our network. The neuron processing the bottom left corner of an image probably doesn't care about the value of the top right corner of the same image. At least not in the early stages.\n",
    "\n",
    "The next step to be able to implement convolutions in our neural networks is to make sure we can \"slide\" our kernel across the entire image. Remember that in this case the kernel is a matrix (for 2D images) or a vector (for 1D data like time-series). So if we force each output neuron to make local connections with the input, we can get our matrix/vector. By forcing each output neuron to use the exact same weights as their neighbouts then we guarantee that they all use the same kernel.\n",
    "\n",
    "![](fig95.png)\n",
    "\n",
    "We call this _weight sharing_. The top diagram in the image above has bolded some of the weights that are shared. You can see that each of the output neurons applies the same weight to the input neuron right across from itself. In contrast, a fully connected network will not share weights.\n",
    "\n",
    "This again allows us to drastically reduce the number of weights we need to keep track of. In fact CNNs are much more efficient for data that has the local correlation we've been talking about since they have much fewer parameters to train.\n",
    "\n",
    "By making use of these local connections and weight sharing we can now implement convolutions in out neural networks. The local connections allow us to formulate our kernel shape, and the weight sharing guarantees that we \"slide\" the same kernel across the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Maps\n",
    "\n",
    "A feature map is the result of our convolutional layer. If we are dealing with images, then the feature map for a particular kernel can be interpreted as an image showing where those features are the most prominent.\n",
    "\n",
    "For example, a kernel that selects for veritcal lines will produce a feature map that will only highlight the vertical features in our image. Whereas a feature map that is generated from a kernel that is selective for right leaning curves will highlight those features in our image.\n",
    "\n",
    "![](featureMaps.png)\n",
    "\n",
    "This does imply that we want to have more than one kernel in each convolutinoal layer. In the first layer of our CNNS we might have a need to know about horizontal lines, vertical lines, left curves, right curves, different line stops, etc. We will not be telling the network what to learn, but we do know that it will need to learn more than one feature!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "The last thing we need for a moden convolutinal network is some form of _pooling_. Pooling is any local statistical summary we can make of our feature maps. For example, we might choose to only take the average of a 2x2 grid into our next layer. This is called average pooling. By far the most common kind of pooling is Max Pooling, where we will take the maximum value from the window in question. Pooling give our network a really nice property: it makes our network _invariant to small shifts_ in the input.\n",
    "\n",
    "![](fig98.png)\n",
    "\n",
    "Since we are making a local summary of the neuron activations at that layer, if we shift the image (or the values of our neurons) one pixel over to the right, most of the outputs from the pooling layer will not change by much (if at all, as is sometimes the case for max pooling). This means that our network does not care if the input is shifted over. Pooling layers also reduce the size of our feature maps, since we are applying some kind of statistical summary, the size of our feature maps tends to at minimum halve in each dimension. This again gives us great gains in computational complexity, since we will have to calculate fewer parameters for subsequent layers. Pooling is also an operation that was inspired by real neuroscience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Considerations\n",
    "\n",
    "You might be concerned that if only allow local connections that we might need to have pretty large kernels, but in reality most models use kernels that are around 5x5 pixels, often 3x3 sometimes as high as 13x13 (very rare). The reason why we can get away with such small kernels is two-fold: stacking convolutional layers, and also the pooling operation make the receptive fields of neurons in later layers larger (see image below).\n",
    "\n",
    "![](fig94.png)\n",
    "\n",
    "So by weaving convolutions and pooling, we can make sure our later layer neurons have a pretty good idea of what features were represented where in the image.\n",
    "\n",
    "We also need to talk about how all these opereations are performed in the context of neural networks. The image below has a schematic.\n",
    "\n",
    "![](fig97.png)\n",
    "\n",
    "We first convolve with as many filters as we have, then apply our non-linearity and finally do some pooling operation. This is not strictly necessary. In fact, there's been some strong papers showing that pooling might not always be necessary. However, the non-linearity must always be present!\n",
    "\n",
    "Finally, at some point you will have to classify the input image. Usually this means that we will need a vector output (a vector with as many elements as there are classes in your problem set). Traditionally, we think of the CNN as being a feature extractor that we will connect to an MLP. That is it will find good features that will be meaningful for our multilayer perceptron. Usually this means that when we are done with our convolutions we will flatten the features maps into a vector and start using fully connected dense layers until we get to out output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras for CNNs\n",
    "\n",
    "Thankfully Keras already has all the necessary layers to be able to do convolutions. We'll be focusing on images almost exclusively from now on, so we'll deal with 2 dimensional convolutions.\n",
    "\n",
    "To make a 2D convolutional layer, we just need to import it, then when adding it to the model set the number of kernels, the size of the kernels, and what non-linearity we want to follow it.\n",
    "\n",
    "We can also import different pooling layers. We will use max pooling for now. When you add it to the model, you specify what size of a window you want to pool over. The default value will reduce the dimensionality of your images by 2 in width and height.\n",
    "\n",
    "As always, you have to remember to give the first layer of your model the input shape of your data, however for images you have to be a little bit careful. There are two different valid image shapes in the wild: (# of Channels, Height, Width) or (Height, Width , # of channels). The number of channels refers to how many color channels you have in your image. Generally you'll have a red, a green and a blue channel. Sometimes it will be better to use a single grayscale channel. In some specialized situations you might have access to light outside the visible spectrum, or to depth information. In any case, you need to let the Conv2D layers know what is the shape of your data. I recommend massaging your data so that the channels are last, as this will be the default in Keras. In any case it is a parameter that you can set.\n",
    "\n",
    "I strongly encourage you to read the [convolution](https://keras.io/layers/convolutional/) and [pooling](https://keras.io/layers/pooling/) documentation on the Keras website. Especially the Conv2D and MaxPooling2D layers.\n",
    "\n",
    "After you are done with your convolutions you can use the Flatten layer to turn your feature maps into a vector and start adding Dense layers into your network until you reach your output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "The exercise for this session will be more involved on the programming side of things than in previous ones. You need to start becoming familiar with Keras and all the errors that will invariably (don't worry we all crash stuff all the time!).\n",
    "\n",
    "For this exercise your job will be to construct a model that will classify the digits of the MNIST dataset. The dataset can be downloaded from keras by going to their datasets module and importing it (this will be done for you). \n",
    "\n",
    "For this task you will not be classifying only two classes but ten (one for each digit). For this reason, your final layer should have 10 output neurons. To make the task simpler, you should use a softmax activation in the last layer, and train using categorical cross entropy as your loss function. Feel free to pick whatever optimizer you want.\n",
    "\n",
    "You will train your model on the training set and then test it with the test set. Do not be discouraged if Keras says that each epoch will take on the order of three to five minutes to train, this is normal. You should expect results above 95% by the end of the second epoch if not the third. There are many trick you can use to reduce training time at the cost of accuracy, feel free to explore.\n",
    "\n",
    "As a bonus, plot some of the kernels from the first layer of your CNN, and some of the associated feature maps, see if you can figure out what that kernel is selecting for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.datasets import mnist\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y_train and y_test into one hot encoded vectors\n",
    "if y_train.shape != (6000,10) :\n",
    "    y_train = utils.to_categorical(y_train, num_classes=10)\n",
    "    \n",
    "if y_test.shape != (1000,10) :\n",
    "    y_test = utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you will need to turn the labels (y) into a one hot encoded vector. Keras has a function in the utils module to do this. You can (and should) plot some the images from the training set, so you can see what you are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25bff517080>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also be really useful to normalize your data. Since these are grayscale images, we know the max is 255 and the min is 0, so dividing the images by 255 will guarantee that all the pixels are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize your training set (and the testing set!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = x_train/255\n",
    "\n",
    "x_test = x_test/255\n",
    "\n",
    "#min = 0, max = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should build and test your model. You can use model.summary() after you have built your model to get a nice summary of what are the output shapes of each layer and how many parameters you have to train. A really common error at this stage is to do too many convolutional-pooling layers such that you try to shrink your input's size too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#first layer (convolution)\n",
    "model.add(Conv2D(16,kernel_size=(3,3),data_format=\"channels_last\",activation='relu',input_shape=(28,28,1)))\n",
    "model.add(Dropout(0.2))\n",
    "#first layer (pooling)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second layer (convolution)\n",
    "model.add(Conv2D(32,kernel_size=(3,3),data_format=\"channels_last\",activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#second layer (pooling)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.8589 - acc: 0.7180 - val_loss: 0.1896 - val_acc: 0.9558\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 0.3277 - acc: 0.9086 - val_loss: 0.1107 - val_acc: 0.9705\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 0.2406 - acc: 0.9351 - val_loss: 0.0930 - val_acc: 0.9775\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.2003 - acc: 0.9464 - val_loss: 0.0714 - val_acc: 0.9814\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.1715 - acc: 0.9543 - val_loss: 0.0624 - val_acc: 0.9849\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.1575 - acc: 0.9595 - val_loss: 0.0543 - val_acc: 0.9860\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.1450 - acc: 0.9618 - val_loss: 0.0553 - val_acc: 0.9852\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.1342 - acc: 0.9647 - val_loss: 0.0523 - val_acc: 0.9854\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.1269 - acc: 0.9668 - val_loss: 0.0464 - val_acc: 0.9858\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.1198 - acc: 0.9687 - val_loss: 0.0434 - val_acc: 0.9882\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 0.1208 - acc: 0.9686 - val_loss: 0.0467 - val_acc: 0.9862\n",
      "Epoch 12/30\n",
      "57984/60000 [===========================>..] - ETA: 2s - loss: 0.1149 - acc: 0.9694"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-3651bbce92db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number = 0\n",
      "index =  4807\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.random.randint(0,9999)\n",
    "p = model.predict(x_test[np.newaxis,x,:,:,:])\n",
    "plt.imshow(x_test[x,:,:,0])\n",
    "#y_test[100]\n",
    "print(\"Number =\",np.argmax(p))\n",
    "print(\"index = \",x)\n",
    "#p[0,np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52074"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,59999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9986254"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.argmax(p))\n",
    "p[0,np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2311259e-04, 3.8025062e-06, 4.7658174e-05, 7.9893897e-07,\n",
       "        1.0370639e-04, 1.2467266e-04, 9.9862540e-01, 1.0208732e-07,\n",
       "        9.7035698e-04, 3.9904094e-07]], dtype=float32)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"15Epocs_dropout_adam.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
